---
layout: post
title:  "Hbase的基本简介及安装、配置、使用（一）"
desc: "hive的基本简介及安装、配置、使用（一）"
keywords: "hive的基本简介及安装、配置、使用（一）,hbase,kinglyjn"
date: 2017-08-26
categories: [Java]
tags: [java]
icon: fa-coffee
---



### Hbase简介

> Hbase是一个构建在HDFS之上的、分布式的、可扩展的、面向列存储的开源数据库，是google的bigtable的开源实现，它主要用于存储海量数据，是hadoop生态系统中重要的一员。

<br>

<img src="http://img.blog.csdn.net/20170826135508330?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:80%"/>

hbase的优点：

* 成熟：社区成熟、理论充分经过实践、丰富的工具支持
* 高效：高并发写入、均衡效果好、null数据的列不占存储空间
* 分布式特性：基于hdfs（数据安全、普通商用服务器廉价、易扩展）、zookeeper
* 表的特点：
  * 大：存储量大，一张表可以有数百亿行、上百万列；检索速度快，准实时，[毫]秒级别；
  * 数据安全：基于hdfs存储，一条记录至少有3个备份
  * 面向列存储：面向列（族）的存储和权限访问，列（族）独立索引；
  * 稀疏：对于为空的列，并不占用存储空间，因此表可以设计的非常稀疏；
  * 数据类型单一：hbase中数据类型都是字符串类型（string）；
  * 无模式：每一行都有一个可以排序的主键和任意多的列，可以根据需要动态增加，同一张表中可以有截然不同的列；

<br>



### hbase数据存储的逻辑模型和物理模型

**数据存储的逻辑模型**

主键（Row Key）：用来检索记录的主键，访问Hbase表中的行，只有三种方式，即通过单个Row key访问、通过Row Key的range 和 全表扫描。主键为任意字符串，最大长度为64kb，按照字典顺序存储，在hbase内部保存为字节数组。

列族（Column Family）：列族在创建表的时候声明，它是一些列的集合，一个列族的所有的列都有相同的前缀，如courses:hisotry和courses:month都是列族courses的成员，冒号(:)是列族的分隔符，用来区分前缀和列名。一个列族可以包含多个列。列中的数据都是以二进制的形式存在，没有数据类型。

时间戳和存储单元（Timestamp and Cell）：Hbase中通过row和columns确定的为一个存储单元称为Cell。每个cell保存着一份数据的多个版本。在写入数据的时候，时间戳可以由hbase自动赋值（当前系统时间精确到毫秒），也可以显式赋值。

<img src="http://img.blog.csdn.net/20170826133710173?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:80%"/>

<img src="http://img.blog.csdn.net/20170826135302496?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:70%"/>

<img src="http://img.blog.csdn.net/20170826135313881?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:70%"/>

<br>

**数据存储的物理模型**

1. table中的所有行都按照rowkey的字典序进行排列；

2. table在行的方向上分割为多个region；<img src="http://img.blog.csdn.net/20170826173248598?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:70%"/>

3. region按大小进行分割，每张表开始只有一个region，随着数据的增多，region不断增大，当增大到一定阈值的时候，region就会分裂为两个新的region，随后会有越来越多的region；<img src="http://img.blog.csdn.net/20170826173801054?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:55%"/>

   ```default
   开始创建表
   --
   ...      分裂
   --    --------->   --          分裂        
                      ...     ----------->   ...
                      300001
             
             
                      300002
                      ...
                      --
   ```

4. Region是hbase中分布式存储和负载均衡的最小单元，不同的region分布到不同的regionserver上；<img src="http://img.blog.csdn.net/20170826174226363?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:57%"/>

5. Region（[startkey, stopkey)）虽是分布式存储的最小单元，但并不是存储的最小单元。

   region由一个或者多个store，每个store保存一个columns family。

   每个store又由一个memStore和0个或多个storeFile组成。

   memStore存储在内存中，storeFile存储在HDFS上。<img src="http://img.blog.csdn.net/20170826174902139?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:60%"/>

   hbase数据写入的流程：

   ```shell
   put --> cell
   	0) 将操作记录写入HDFS上的WAL预写日志
   	1) 将数据先写入内存memstore中
   	2) 当memstore达到一定阈值的时候，将数据写入到磁盘storeFile中
   ```

   ​

<br>



### 搭建hbase伪分布式式环境

[参考](http://hbase.apache.org/book.html#quickstart)

```shell
# 1. 关闭防火墙(卸载防火墙apt-get remove iptables)，关闭selinux
ufw disable

# 2. 下载解压(或者编译源码)
# 可以参考 http://archive.cloudera.com/cdh5/cdh/5/ 来确定cdh生态对应的版本
wget http://archive.apache.org/dist/hbase/hbase-0.98.6/hbase-0.98.6-hadoop2-bin.tar.gz

# 3. 官方默认的是以hadoop2.2.0编译的，这里我们需要手动替换habsehome/lib文件夹中hadoop相关的jar成我们需要的hadoop的版本(2.5.0)
需要替换的jar包如下：
hadoop-annotations-2.2.0.jar
hadoop-auth-2.2.0.jar
hadoop-client-2.2.0.jar
hadoop-common-2.2.0.jar
hadoop-hdfs-2.2.0.jar
hadoop-mapreduce-client-app-2.2.0.jar
hadoop-mapreduce-client-common-2.2.0.jar
hadoop-mapreduce-client-core-2.2.0.jar
hadoop-mapreduce-client-jobclient-2.2.0.jar
hadoop-mapreduce-client-shuffle-2.2.0.jar
hadoop-yarn-api-2.2.0.jar
hadoop-yarn-client-2.2.0.jar
hadoop-yarn-common-2.2.0.jar
hadoop-yarn-server-common-2.2.0.jar
hadoop-yarn-server-nodemanager-2.2.0.jar

# 4. 修改HBase的配置文件 conf/hbase-env.sh 修改内容如下：
export JAVA_HOME=/opt/modules/jdk1.8.0_144
export HBASE_MANAGES_ZK=false   #HBase是否管理它自己的ZooKeeper的实例

# 5. 配置文件
# 5.1 conf/hbase-site.xml
# [注] 如果在 HDFS HA 上部署HBase需要做的更改如下：
# 在hbase-site.xml中，rootdir改为和hadoop的dfs.nameservices一样(e.g. hdfs://ns1/hbase)
# 将hdfs的配置文件core-site.xml和hdfs-site.xml拷贝到hbase的conf目录下，然后重启hbase
-------------------------
<property>
  	<name>hbase.cluster.distributed</name>
  	<value>true</value>
</property>
<property>
  	<name>hbase.rootdir</name>
  	<value>hdfs://nimbusz:8020/hbase</value>
</property>
<property>
  	<name>hbase.zookeeper.quorum</name>
  	<value>nimbusz,supervisor01z,supervisor02z</value>
</property>
<property>
  	<name>hbase.zookeeper.property.clientPort</name>
  	<value>2181</value>
</property>

# 5.2 conf/regionservers(一台机器的伪分布式)
------------------------
nimbusz


# 6. 快捷启动hbase
bin/start-hbase.sh
或
bin/hbase-daemon.sh start master
bin/hbase-daemon.sh start regionserver

# 7. 查看hbase相关进程
35216 HMaster
11009 QuorumPeerMain
35362 HRegionServer

# 8. web查看hbase集群
curl http://nimbusz:60010/master-status

# 9. 关闭hbase
bin/stop-hbase.sh
或
bin/hbase-daemon.sh stop regionserver
bin/hbase-daemon.sh stop master
```

<br>



### hbase.rootdir 对应HDFS文件简介

<img src="http://img.blog.csdn.net/20170826162159029?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="100%"/>

<br>



### Hbase shell

```shell
# 查看hbase脚本提供的一些功能
$ bin/hbase
Usage: hbase [<options>] <command> [<args>]
Options:
  --config DIR    Configuration direction to use. Default: ./conf
  --hosts HOSTS   Override the list in 'regionservers' file
Commands:
Some commands take arguments. Pass no args or -h for usage.
  shell           Run the HBase shell
  hbck            Run the hbase 'fsck' tool
  hlog            Write-ahead-log analyzer
  hfile           Store file analyzer
  zkcli           Run the ZooKeeper shell
  upgrade         Upgrade hbase
  master          Run an HBase HMaster node
  regionserver    Run an HBase HRegionServer node
  zookeeper       Run a Zookeeper server
  rest            Run an HBase REST server
  thrift          Run the HBase Thrift server
  thrift2         Run the HBase Thrift2 server
  clean           Run the HBase clean up script
  classpath       Dump hbase CLASSPATH
  mapredcp        Dump CLASSPATH entries required by mapreduce
  pe              Run PerformanceEvaluation
  ltt             Run LoadTestTool
  version         Print the version
  CLASSNAME       Run the class named CLASSNAME
  
  
# 使用hbase shell
$ bin/habse shell

# 使用help查看帮助文档
hbase(main):001:0> help
hbase(main):001:0> help "list"
```

<br>



### Hbase shell 常见命令

```sql
-- 查看数据库中有哪些表
list

-- 创建表(单引号和双引号都可以)
create 'user', 'info'

-- 查看表的描述信息
describe 'user'

-- 插入数据
put 'user', '1001', 'info:name', 'zhangsan'
put 'user', '1001', 'info:age', '25'
put 'user', '1001', 'info:gender', '1'
put 'user', '1001', 'info:address', 'beijing'


-- 查询表的数据
-- 在hbase中的查询只有三种方式
-- 1. 依据rowkey进行查询，最快
get 'user', '1001'
get 'user', '1001', 'info:name'

-- 2. 范围查询，用的最多
scan 'user', {COLUMNS => ['info:name','info:age'], LIMIT => 2, STARTROW => '1001'}

-- 3. 全表扫描，最慢
scan 'user'


-- 删除表的数据
-- 1. 删除某条数据的一列
delete 'user', '1001', 'info:age'

-- 2. 删除某条数据的全部列
deleteadll 'user', '1001'
```

 <br>