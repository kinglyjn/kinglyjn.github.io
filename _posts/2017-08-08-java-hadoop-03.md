---
layout: post
title:  "hadoop完全分布式环境的搭建以及HDFS和YARN的HA配置（三）"
desc: "hadoop完全分布式环境的搭建以及HDFS和YARN的HA配置（三）"
keywords: "hadoop完全分布式环境的搭建以及HDFS和YARN的HA配置（三）,hadoop,kinglyjn"
date: 2017-08-08
categories: [Java]
tags: [java]
icon: fa-coffee
---



### 前期准备

对机器的配置

|      | numbusz | supervisor01z | supervisor02z |
| ---- | ------- | ------------- | ------------- |
| CPU  | 20核     | 20核           | 20核           |
| 内存   | 50G     | 50G           | 50G           |
| 硬盘   | 100TB   | 100TB         | 100TB         |

对机器的服务进行规划

```dedault
numbusz                supervisor01z                supervisor02z             备注
-----------------------------------------------------------------------------------------
namenode              secondarynamenode             redourcemanager         
datanode              datanode                      datanode                主要消耗硬盘
nodemanager           nodemanager                   nodemanager             主要消耗CPU            
historyserver
```



>  常见的hadoop的默认web端口：
>
>  yarn外部管理界面端口：8088
>
>  HDFS外部管理界面端口：50070
>
>  secondarynamenode外部管理界面端口：50090

<br>



### 配置和搭建过程

```shell
#0. 设置各个节点之间的ssh双向无密码登录
#为了统一起见，这里我们在都在/opt目录下创建统一文件夹方便安装和管理
#并且安装hadoop 和 jdk统一在 /opt/modules 目录下
#当在一台主机上所有的安装配置做完之后，只需要使用scp命令进行文件夹的分发即可
$ cd /opt
$ sudo mkdir -p datas  modules  software  tools
$ sudo chown -R ubuntu:ubuntu datas  modules  software  tools

#1. 在一台机器上（比如nimbusz）解压hadoop，删除没用的文档文件
$ tar -zxvf hadoop-2.5.0.tar.gz -C /opt/modules
$ rm -rf /opt/modules/hadoop-2.5.0/share/doc

#2. 配置基本 *.env 环境的java环境，主要配置以下三个配置文件，并查看是否生效：
hadoop-env.sh
mapred-env.sh
yarn-env.sh
$ bin/hadoop


#3. 配置 *.site.xml文件，主要包含以下4+1个配置文件：
core-site.xml
hdfs-site.xml
slaves
mapred-site.xml
yarn-site.xml

#3.1 配置core-site.xml
<configuration>
	<!--hdfs主从节点通信默认端口号为8020-->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nimbusz:8020</value>
    </property>
    
    <!--hdfs的namenode节点元数据的镜像文件-->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/modules/hadoop-2.5.0/data/tmp</value>
    </property>
    <property>
        <!--设置一个hadoop的web静态用户名(默认为dr.who)-->
        <name>hadoop.http.staticuser.user</name>
        <value>ubuntu</value>
    </property>
    
    <!--hdfs垃圾箱设置，单位为分钟-->
    <property>
        <name>fs.trash.interval</name>
        <value>1440</value>
    </property>
    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>1440</value>
    </property>
</configuration>

#3.2 配置hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>supervisor01z:50090</value>
    </property>
</configuration>

#3.3 配置slaves
#严格来讲，slaves文件存储的并不是真正的datanode节点，它唯一的作用是在集群操作的时候要正对这个文件去ssh处理。我们在start-all.sh、start-dfs.sh、start-yarn.sh的时候，通过ssh向远程服务器发送指令，将远程服务器启动。至于节点是不是datanode节点，取决于dfs.hosts和dfs.hosts.exclude两个参数。
nimbusz
supervisor01z
supervisor02z

#3.4 配置mapred-site.xml
<configuration>
	<!--指定mapreduce运行在yarn上-->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>nimbusz:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>nimbusz:19888</value>
    </property>
</configuration>

#3.5 配置yarn-site.xml
<configuration>
	<!--启动mapreduce_shuffle服务以支持运行mapreduce程序-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!--指定resourcemanager所在的机器-->
    <!--如果没有指定，则在集群的哪一台机器启动resourcemanager，则就认为哪一台机器就是RMr-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>supervisor02z</value>
    </property>	
    <!--如果有多张网卡，需要配置此选项，才能打开web界面-->
    <property>
    	<name>yarn.resourcemanager.webapp.address</name>
    	<value>master:8088</value>
    </property>
    <property>
        <!--启用hadoop的日志聚集功能(默认false)-->
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <!--日志保存在HDFS上的期限(默认为-1，这里配置的是一周)-->
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>


#4. 将配置好的jdk和hadoop环境分发到各个节点，并在各个节点上配置好jdk和hadoop的环境变量
$ sudo scp datas  modules  software  tools ubuntu@supervsior01z:/opt
$ sudo scp datas  modules  software  tools ubuntu@supervsior02z:/opt
$ vim /etc/profile
export JAVA_HOME=/opt/modules/jdk1.8.0_144
export PATH=$PATH:$JAVA_HOME/bin


#5. 分别在各个节点初始格式化namenode
$ bin/hdfs namenode -format


#6. 在nimbusz启动namenode、在supervisor01z启动secondarynamenode、
#   在nimbusz、supervisor01z、supervisor02z分别启动datanode；
$ sbin/hadoop-daemon.sh start namenode
$ sbin/hadoop-daemon.sh start secondarynamenode
$ sbin/hadoop-daemon.sh start datanode
# 测试HDFS工作正常
$ bin/hdfs dfs -mkdir -p /user/ubuntu/wc/input
$ curl http://nimbusz:50070/explorer.html
$ bin/hdfs dfs -put /opt/datas/wc.txt /user/ubuntu/wc/input
$ bin/hdfs dfs -text /user/ubuntu/wc/input/wc.txt
# 测试secondarynamenode正常
$ curl http://supervisor01z:50090

# 在supervisor02z启动redourcemanager、
# 在nimbusz、supervisor01z、supervisor02z分别启动nodemanager；
# 在nimbusz启动historyserver
$ sbin/yarn-daemon.sh start resourcemanager
$ sbin/yarn-daemon.sh start nodemanager
$ sbin/mr-jobhistory-daemon.sh start historyserver
# 测试MR在yarn上运行正常
$ bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount /user/ubuntu/wc/input /user/ubuntu/wc/output
$ curl http://nimbusz:8088/explorer.html
$ bin/hdfs dfs -text /user/ubuntu/wc/output/part*
```

<br>



### 快捷启动和关闭hadoop集群

```shell
# 前提是已经设置各个节点之间的ssh双向无密码登录
#1.1 在任意一个节点上关闭HDFS，执行下面命令将会
#关闭nimbusz的namenode，关闭supervisor01z的secondarynamenode，关闭各个节点的datanode
$ sbin/stop-dfs.sh 

#1.2 在任意一个节点上关闭yarn，执行下面的命令将会
#关闭supervisor02z的resourcemanager，关闭各个节点的nodemanager
$ sbin/stop-yarn.sh 

#1.3 在nimbusz上关闭historyserver
$ sbin/mr-jobhistory-daemon.sh stop historyserver


#启动hadoop集群
#2.1 在nimbusz节点执行
$ sbin/start-dfs.sh

#2.2 在supervisor02z节点执行
$ sbin/start-yarn.sh 

#2.3 在nimbusz上执行
$ sbin/mr-jobhistory-daemon.sh start historyserver
```

<br>



### 集群的时间同步服务（NTP服务）

1. 由于linux服务器运行时间久了会造成时间上的误差，需要我们配置各个节点之间的时间同步服务。
2. 国家授时中心，可以通过互联网去连接这些时间同步服务器，但是如果不能联网，只能通过自己的内部服务器实现时间同步功能
3. 内网时间同步可以通过在内网集群中找到一台机器作为时间同步基准服务器

以下是ubuntu系统安装和配置时间同步服务的步骤：(以下是使用ntp作为基准服务器，其他节点使用ntpdate定时同步ntp基准服务器的时间；由于ntp既是服务器，也可以作为客户端，所以也可以只使用ntp完成时间同步的需求，这也是推荐的做法)

```shell
#1.1 在基准节点安装ntp服务(比如我们在nimbusz，这里安装的时候要确保联网)
$ sudo vim 
$ sudo apt-get install ntp

#1.2 安装完成之后我们来看一下服务是否启动，[+]表示已启动
$ service --status-all |grep ntp
[ + ]  ntp

#1.3 修改ntp服务的配置文件/etc/ntp.conf
$ sudo vim /etc/ntp.conf
	#第一处：允许这个网段的对时请求
    restrict 172.16.127.0 mask 255.255.255.0 nomodify notrap
    
    #第二处：由于是内网环境，就不需要这些服务配置，注释掉
    #server 0.ubuntu.pool.ntp.org
    #server 1.ubuntu.pool.ntp.org...
    
    #第三处：ntp server 提供的本地服务
    #让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的
    #server都不可用时，将使用local时间作为ntp服务提供给ntp客户端
    #上层ntp server，在客户端打开这个选项
    #server nimbusz prefer
    server 127.127.1.0
    fudge 127.127.1.0 stratum 10
    
#1.4 重启ntp服务，并设置ntp自动开机重启
$ sudo service ntp restart
$ sudo sysv-rc-conf


#2.1 在其他需要同步基准服务器的节点上，计划一个crontab，用来进行时间同步(这里使用sudo，也可直接切su -)
$ sudo crontab -e
	#sync time
	0-59/10 * * * * /usr/sbin/ntpdate nimbusz
$ sudo crontab -l
$ sudo ntpdate nimbusz
$ sudo hwclock --systohc #同步系统时间到硬件
$ sudo hwclock --show #查看硬件时间

#2.2 除了同步系统时间，在设置同步BIOS的时间：
hwclock --show 		显示bios时间
hwclock --systohc 	将系统时间写入bios
hwclock --hctosys 	将bios时间写入系统
hwclock --help 		显示帮助
```

> 注：安装sysv-rc-conf启动项管理服务
>
> $ sudo apt-get install sysv-rc-conf
>
> $ sudo sysv-rc-conf
>
> 
>
> 例如在大多数linux操作系统下一共有如下6个典型的运行级别：
>
> 0 停机
>
> 1 单用户，Does not configure network interfaces, start daemons, or allow non-root logins
>
> 2 多用户，无网络连接 Does not configure network interfaces or start daemons
>
> 3 多用户，启动网络连接 Starts the system normally.
>
> 4 用户自定义
>
> 5 多用户带图形界面
>
> 6 重启
>
> 查看当前系统的运行级别可以使用命令：$ runlevel
>
> 切换运行级别：init [0123456Ss]  如：用 init 0 命令关机；用 init 6 命令重新启动
>
> 
>
> Linux 系统主要启动步骤：
>
> ```default
> 1. 读取 MBR 的信息,启动 Boot Manager
>         Windows 使用 NTLDR 作为 Boot Manager,如果您的系统中安装多个
>         版本的 Windows,您就需要在 NTLDR 中选择您要进入的系统。
>         Linux 通常使用功能强大,配置灵活的 GRUB 作为 Boot Manager。
> 2. 加载系统内核,启动 init 进程
>         init 进程是 Linux 的根进程,所有的系统进程都是它的子进程。
> 3. init 进程读取 /etc/inittab 文件中的信息,并进入预设的运行级别,
>    按顺序运行该运行级别对应文件夹下的脚本。脚本通常以 start 参数启
>    动,并指向一个系统中的程序。
>         通常情况下, /etc/rcS.d/ 目录下的启动脚本首先被执行,然后是
>         /etc/rcN.d/ 目录。例如您设定的运行级别为 3,那么它对应的启动
>         目录为 /etc/rc3.d/ 。
> 4. 根据 /etc/rcS.d/ 文件夹中对应的脚本启动 Xwindow 服务器 xorg
>         Xwindow 为 Linux 下的图形用户界面系统。
> 5. 启动登录管理器,等待用户登录
>         Ubuntu 系统默认使用 GDM 作为登录管理器,您在登录管理器界面中
>         输入用户名和密码后,便可以登录系统。(您可以在 /etc/rc3.d/
>         文件夹中找到一个名为 S13gdm 的链接)
> ```

<br>



### ZK

* 分布式架构
* 功能：同步&选举
* 节点数目：2n+1（n为允许宕机的机器）
* 数据结构：节点树的结构，每个节点都是一个znode，存放在内存中，本地磁盘中会有备份。

下面是zk的基本配置：可 [参考](http://www.page.keyllo.com/linux/2017/07/26/zookeeper-base01.html)

```shell
tickTime=2000
initLimit=10
syncLimit=5

dataDir=/home/ubuntu/zookeeper-3.4.6/zkdata
dataLogDir=/home/ubuntu/zookeeper-3.4.6/logs
clientPort=2181

server.1=nimbusz:2888:3888
server.2=supervisor01z:2888:3888
server.3=supervisor02z:2888:3888
```

<br>



### 解决hdfs的单点故障-HDFS HA 架构

在hadoop2.0之前，namenode存在单点故障（SPOF），对于只有一个namenode节点的hdfs集群，当namenode宕机之后，则整个集群将无法使用，直到namenode重新启动。HA架构通常有两个namenode。

1. 元数据同步需要保证两个namenode内存中存储的文件系统元数据是一致的

   思路：namenode的启动过程

   * 读取fsimage和edits文件，读取后生成新的fsimage和edits文件
   * 另一个namenode同样需要去读取这两个文件，变化后的edits文件同样需要读取
   * 注册心跳、块的报告也需要向另一个namenode实时地进行汇报

2. 日志文件的安全性

   * cloudera公司提出了分布式日志存储方案
   * 找到一个datanode节点的目录，只要日志在2n+1个节点的n+1个节点存储成功即可，journalnode日志节点。这时候seconarynamenode就没有存在的必要了，HA架构中不能使用secondarynamenode！
   * 日志文件被写了多份存储在HDFS上，为了保证高可用，datanode需要是奇数个
   * 当然日志文件也可以存储在zk上，但是不适合日志量很大的情形

3. 通过代理来让客户端判断现在对外提供服务的是哪一台namenode。需要保证同一时间下，对外提供服务的只能是其中一个namenode节点，这里会涉及到一个`隔离的方法`。



手动版的 HDFS HA 配置、启动、使用过程：[参考](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html)

```shell
#配置HDFS HA的要点：
#编辑日志文件（share edits），通过journalnode节点配置（最少3台）
#配置两个namenode（active、standby）
#HDFS客户端代理（client proxy）
#隔离（fence），同一时刻仅仅有一个namenode对外提供服务

#0.0 HDFS HA 集群的规划：
nimbusz             supervisor01z           supervisor02z
---------------------------------------------------------
journalnode         journalnode             journalnode
namenode（active）   namenode（standby）						
datanode            datanode                datanode



#1.1 core-site.xml
<configuration>
	<property>
		<!--执行HDFS的命名空间-->
     	<name>fs.defaultFS</name>
      	<value>hdfs://ns1</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/modules/hadoop-2.5.0/data/tmp</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>ubuntu</value>
    </property>
</configuration>


#1.2 hdfs-site.xml
<configuration>
	<property>
		<!--指定命nameservices的名空间的名称-->
      	<name>dfs.nameservices</name>
      	<value>ns1</value>
    </property>
    <property>
    	<!--指定两个namenode的名称-->
      	<name>dfs.ha.namenodes.ns1</name>
      	<value>nn1,nn2</value>
    </property>
    <property>
    	<!--指定nn1的主机位置-->
      	<name>dfs.namenode.rpc-address.ns1.nn1</name>
        <value>nimbusz:8020</value>
    </property>
    <property>
    	<!--指定nn2的主机位置-->
      	<name>dfs.namenode.rpc-address.ns1.nn2</name>
      	<value>supervisor01z:8020</value>
    </property>
    <property>
    	<!--指定nn1的web访问主机位置-->
     	<name>dfs.namenode.http-address.ns1.nn1</name>
      	<value>nimbusz:50070</value>
    </property>
    <property>
    	<!--指定nn2的web访问主机位置-->
      	<name>dfs.namenode.http-address.ns1.nn2</name>
      	<value>supervisor01z:50070</value>
    </property>
    <property>
    	<!--指定日志节点的主机、端口和命名空间-->
     	<name>dfs.namenode.shared.edits.dir</name>
      	<value>qjournal://nimbusz:8485;supervisor01z:8485;supervisor02z:8485/ns1</value>
    </property>
    <property>
    	<!--指定日志节点真正存储的路径-->
    	<name>dfs.journalnode.edits.dir</name>
    	<value>/opt/modules/hadoop-2.5.0/data/jn</value>
    </property>
    <property>
    	<!--指定客户端代理访问的配置-->
      	<name>dfs.client.failover.proxy.provider.ns1</name>
      	<value>org.apache.hadoop.hdfs.server.namenode.ha
      			.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
    	<!--使用ssh协议的方式将两个namenode进行隔离-->
      	<name>dfs.ha.fencing.methods</name>
      	<value>sshfence</value>
    </property>
    <property>
    	<!--使用ssh协议的方式将两个namenode进行隔离-->
      	<name>dfs.ha.fencing.ssh.private-key-files</name>
      	<value>/home/ubuntu/.ssh/id_rsa</value>
    </property>
    
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>


#2.1 先分别启动三个journalnode节点
# 注：在各个JournalNode机器上执行命令“hadoop-daemon.sh  journalnode”。
# 如果是一个新的HDFS集群，还要首先执行格式化命令“hdfs  namenode  -format”，紧接着启动本NameNode进程。
# 如果存在一个已经格式化过的NameNode，并且已经启动了。那么应该把该NameNode的数据同步到另一个没有格式化的NameNode。在未格式化过的NameNode上执行命令“hdfs  namenode  -bootstrapStandby”。
# 如果是把一个非HA集群转成HA集群，应该运行命令“hdfs –initializeSharedEdits”，这会初始化JournalNode中的数据。
$ sbin/hadoop-daemon.sh start journalnode

#2.2 格式化nimbusz节点的namenode，并启动查看web页面是否为standby状态
$ bin/hdfs namenode -format
$ sbin/hadoop-daemon.sh start namenode
$ curl http://nimbusz:50070

#2.3 在supervisor01z节点同步nimbusz节点的namenode的状态，并启动supervisor01z的namenode节点
$ bin/hdfs namenode -help
$ bin/hdfs namenode -bootstrapStandby
$ sbin/hadoop-daemon.sh start namenode

#2.4 分别启动3个datanode
$ sbin/hadoop-daemon.sh start datanode

#2.5 到目前为止，两个nimbusz和supervisor01z这两个namenode都是standby待命状态的namenode，需要
#使用haadmin命令将其中一个设置为active工作状态(这里将nimbusz设置active状态)
$ bin/hdfs haadmin -transitionToActive nn1


#3.1 测试，创建文件夹，上传文件，读取文件
$ bin/hdfs dfs -mkdir -p /user/ubuntu/wc/input
$ bin/hdfs dfs -put /opt/datas/wc.txt /user/ubuntu/wc/input
$ bin/hdfs dfs -text /opt/datas/wc.txt /user/ubuntu/wc/input/wc.txt

#3.2 现在将nimbusz节点的namenode杀掉(假设pid为18477，模拟namenode宕机)，
# 手动切换supervisor01z的namenode为active状态，看namenode的HA是否生效
$ kill -9 18477 										  #在nimbusz节点操作
$ bin/hdfs haadmin -transitionToActive nn2 --forceactive  #在supervisor01z节点操作
$ bin/hdfs dfs -text /opt/datas/wc.txt /user/ubuntu/wc/input/wc.txt
```

> 注：这里也可以使用hadoop提供的快捷情动和关闭命令
>
> $ sbin/start-dfs.sh     #分别在对应的机器上启动namenode、datanode、journalnode
>
> $ sbin/stop-dfs.sh     #分别在对应的机器上关闭namenode、datanode、journalnode

<br>



### HDFS HA 自动故障转移 - ZKFC（ZK故障转移监听器）

上述我们配置的 HDFS HA，当active的namenode发生故障宕机时，我们需要手动将另外的namenode切换为namenode工作节点。那么hadoop有没有提供一种机制，当namenode发生故障的时候，自动切换工作namenode呢？这就要用到 HDFS的 ZKFC机制。下面是 HDFS HA 集群自动故障转移的配置：[参考](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)

```shell
#0.0 HDFS HA-ZKFC 集群的规划：
nimbusz             supervisor01z           supervisor02z
---------------------------------------------------------
journalnode         journalnode             journalnode
namenode（active）   namenode（standby）						
datanode            datanode                datanode
zk                  zk                      zk


#1.1 hdfs-site.xml
#增加如下配置参数
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
   	<value>true</value>
</property>
<property>
   	<name>ha.zookeeper.quorum</name>
   	<value>nimbusz:2181,supervisor01z:2181,supervisor02z:2181</value>
</property>


#2.1 启动zk集群(分别在各个zk节点上启动zk服务)
$ bin/zkService.sh start

#2.2 在zk数据树节点上根部生成 hadoop-ha/ns1/[nn1,nn2] 节点
$ bin/hdfs zkfc -formatZK

#2.2 在nimbusz节点上启动 hdfs ha 集群，通过jps我们看到，hadoop会为每个namenode节点设置zkfc监控进程
# 如果需要单独启动zkfc，则可以使用 sbin/hadoop.sh start zkfc 命令
$ sbin/start-dfs.sh
$ jps #nimbusz节点
23521 DataNode
23730 JournalNode
24040 Jps
23917 DFSZKFailoverController
18159 QuorumPeerMain
23375 NameNode

$ jps #supervisor01z节点
24288 DataNode
24400 JournalNode
19537 QuorumPeerMain
24171 NameNode
24636 Jps
24527 DFSZKFailoverController

$ jps #supervisor02z节点
21298 Jps
21203 JournalNode
21084 DataNode
18750 QuorumPeerMain


#3 测试 HDFS HA-ZKFC 集群是否能够完成故障的自动转移
$ bin/hdfs dfs -put /opt/datas/test.txt /user/ubuntu/wc/input
$ kill -9 23375  					#kill namenode in nimbusz
$ curl http://supervisor01z:50070 	#check supervisor01z namenode if it's to active normal
```

自动切换的流程：

1. zkfc通过rpc请求获取namenode状态
2. zkfc连接zk完成选举，保持心跳
3. 另一台zkfc监控本机的namenode状态
4. 由于某种因素，对外提供服务的namenode出现异常宕机
5. zkfc会在下一个监听的周期中检测到namenode失效的信息（zk的临时节点机制），触发后续的故障处理操作
6. zkfc将失效的信息给到zk，zk会与另一台的节点进行通信，standby节点zkfc进入切换的流程
7. zkfc首先将失效的namenode隔离出集群之外
8. zkfc接下来进行选举，确认一个standby节点作为一个新的active节点
9. zkfc完成选举之后，standby节点就切换成了active状态，接管HDFS集群的namenode工作

<br>



### ResourceManager HA 架构

hdfs在没有ha设置之前，namenode存在单点故障，而同样的 yarn resourcemanager 也会存在这样的问题。同样可以结合使用 zk 搭建 resourcemanager ha 的架构，来解决单点故障的问题，更进一步可以完成 resourcemanager的自动故障转移。下面是 resourcemanager ha zkfc 架构的搭建过程：

```shell
#0.0 RsourceManager HA-ZKFC 集群的规划：
nimbusz             supervisor01z            supervisor02z
--------------------------------------------------------------------
                    resourcemanager(active)  resourcemanager(standby)
nodemanager         nodemanager              nodemanager
zk                  zk                       zk


#1.1 yarn-site.xml 
<configuration>
    <property>
        <!--代表是否开启resourcemanager的高可用功能-->
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <!--给resourcemanager集群指定一个id-->
        <name>yarn.resourcemanager.cluster-id</name>
        <value>rs1</value>
    </property>
    <property>
        <!--给resourcemanager两个节点指定的编号-->
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <!--指定节点1的主机-->
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>supervisor01z</value>
    </property>
    <property>
        <!--指定节点2的主机-->
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>supervisor02z</value>
    </property>
    <property>
        <!--指定zk集群的节点-->
        <name>yarn.resourcemanager.zk-address</name>
        <value>nimbusz:2181,supervisor01z:2181,supervisor02z:2181</value>
    </property>
    <property>
        <!--当机器出现故障时候，任务是否重新提交执行-->
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <!--指定resourcemanager的状态信息存储在zk上（默认是存在文件系统上的）-->
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.
        		recovery.ZKRMStateStore</value>
    </property>

    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>


#2.1 启动zk集群(分别在各个zk节点上启动zk服务)
$ bin/zkService.sh start

#2.2 在supervisor01z节点上启动 resourcemanager ha-zkfc 集群
$ sbin/start-yarn.sh

#2.3 由于之前我们已经格式化过namenode，所以这里我们直接启动hdfs ha-zkfc集群
$ sbin/start-dfs.sh 
$ hadoop-2.5.0/sbin/yarn-daemon.sh start resourcemanager

#2.4 观察各个节点上的服务状态
$ jps 	#nimbusz
11252 Jps
10344 NodeManager
10648 NameNode
9753 QuorumPeerMain
10793 DataNode
11001 JournalNode
11194 DFSZKFailoverController

$ jps	#supervisor01z
11376 JournalNode
10673 NodeManager
11508 DFSZKFailoverController
9672 QuorumPeerMain
11626 Jps
10522 ResourceManager
11148 NameNode
11263 DataNode

$ jps	#supervisor02z
10432 DataNode
10544 JournalNode
10644 Jps
10262 ResourceManager
10088 NodeManager
9593 QuorumPeerMain


#2.4 测试MR任务执行的过程中，认为模拟supervisor01z的resourcemanager宕机，观察任务是否正常执行完成
# 如果任务重启并执行完成，说明 resourcemanager ha-zkfc环境搭建成功。
$ bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount /user/ubuntu/wc/input /user/ubuntu/wc/output
$ kill -9 10522

#在上述测试的测试的过程中，我们观察到有下面的一句话输出，说明resourcemanager ha-zkfc环境搭建成功：
#17/08/10 19:50:08 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2
```

<br>



### yarn 管理各个nodemanager的内存和cpu

内存管理 yarn-site.xml

```xml
<!--
表示该节点上YARN可使用的物理内存总量，默认是8G。注意，如果你的节点资源内存不足8，则需要调小这个值，而YARN不会智能地探测到节点的物理内存总量
-->
<property>
	<name>yarn.nodemanager.resource.memory-mb</name>
  	<value>8192</value>
</property>

<!--
单个任务可以申请的最大内存量，默认是8192M，如果一个任务申请的物理内存量大于该值，则将对应的值改为这个数
-->
<property>
	<name>yarn.scheduler.maximum-allocation-mb</name>
  	<value>8192</value>
</property>

<!--
单个任务可以申请的最小内存量，默认是1024M，如果一个任务申请的物理内存量少于该值，则将对应的值改为这个数
-->
<property>
	<name>yarn.scheduler.minimum-allocation-mb</name>
  	<value>1024</value>
</property>

<!--
任务每使用1M物理内存，最多可使用虚拟内存的总量，默认是2.1
-->
<property>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
  	<value>2.1</value>
</property>

<!--
是否启动一个线程检查每个任务使用的物理内存量，如果任务超过分配值，则直接将其杀掉，默认是true
-->
<property>
	<name>yarn.nodemanager.pmem-check-enabled</name>
  	<value>true</value>
</property>

<!--
是否启动一个线程检查每个任务使用的虚拟内存量，如果任务超过分配值，则直接将其杀掉，默认是true
-->
<property>
	<name>yarn.nodemanager.vmem-check-enabled</name>
  	<value>true</value>
</property>
```

<br>

CPU管理 yarn-site.xml

```xml
<!--
表示该节点上yarn可使用的虚拟CPU个数，默认是8。注意，目前推荐将该值设置为与物理cpu核数相同，如果你的节点cpu个数不足8个，则需要调小这个值，而yarn不能智能地探测节点的物理cpu核数
-->
<property>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
  	<value>8</value>
</property>

<!--
单个任务可以申请的最大虚拟CPU个数，默认是32，如果第一个任务申请的cpu个数大于该数，则对应的值改为这个数
-->
<property>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
  	<value>32</value>
</property>

<!--
单个任务可以申请的最小虚拟CPU个数，默认是1，如果第一个任务申请的cpu个数少于该数，则对应的值改为这个数
-->
<property>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
  	<value>1</value>
</property>
```





