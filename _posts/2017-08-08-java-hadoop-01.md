---
layout: post
title:  "hadoop的基本简介及安装、配置、使用（一）"
desc: "hadoop的基本简介及安装、配置、使用（一）"
keywords: "hadoop的基本简介及安装、配置、使用（一）,hadoop,kinglyjn"
date: 2017-08-08
categories: [Java]
tags: [java]
icon: fa-coffee
---



### 大数据的特点

* 大（大象 Volume）
* 繁（章鱼 Variety）
* 快（豹子 Velocity）
* 值（淘金 Value）

<br>



### 由谷歌的三驾马车引申出hadoop

MapReduce  —> Map  & Reduce 计算架构

GFS  —> HDFS分布式文件系统

bigtable —> hbase数据库

<br>



### hadoop的常用版本

HADOOP是什么：可靠的、可扩展的、分布式计算框架。

apache hadoop

cloudera hadoop（CDH版 hadoop）

<br>



### hadoop生态系统

<img src="http://img.blog.csdn.net/20170813113805924?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2luZ2x5am4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" style="width:70%"/>



### hadoop的特点

* 可靠：计算 & 存储：校验码定期检测数据块是否损坏、数据存储在三台不同机器上的三个副本
* 可扩展：集群：机架可插拔、容错性

<br>



### hadoop的核心模块

* Hadoop-Common 
* HDFS
* YARN
* MapReduce

<br>



### hadoop的配置文件

* 默认的配置文件：存储在$HADOOP_HOME/share/hadoop 目录的子目录各个模块的jar包中；
* 自定义的配置文件：存储在$HADOOP_HOME/etc/hadoop 下面；
* 每次hadoop的操作，hadoop都会先加载默认的配置文件，再加载自定义的配置文件，这时候自定义配置文件参数会覆盖默认的配置文件参数，也就是自定义的配置文件的优先级要高于默认的配置文件。

<br>



### HDFS文件系统初步简介

`设计的思想`：一次写入、多次读取

`架构设计`：主从架构

* 主节点（namenode）：管理和存储文件的元数据
* 从节点（datanode）：真正存储文件的

`文件的属性`：名称、位置、副本数、权限、块（大小默认为128M)

文件的块：比如文件的大小为500M，块的最大大小设置为256M，则第一个和第二个块的代销是256M、244M。

* 文件的大小小于数据块的大小，则文件不会占据整个块的空间
* 多个文件不能放到一个块中，即一个块一个文件

`文件的读写流程`：

* 首先客户端从namenode获取文件的位置（RPC协议通信）
* 然后客户端再按照`就近原则`找datanode获取文件的数据

`HDFS启动`：

* namenode在启动的时候会有一个大概30s的等待过程，在这30s内namenode会接收所有datanode的`心跳注册` 和 `块的状态报告`，在这大概30s的时间内，HDFS是处于安全模式的（只读），另外namenode自己也会将fsimage镜像文件和edits操作日志文件中的元信息读取到内存当中
* datanode会周期性向namenode发送心跳（一般每隔3s发送一次），namenode接收到心跳后会给datanode一个反馈，反馈可能会附带一些指令
* 在启动的过程中，所有的datanode向namenode汇报块的状态报告（有哪些块、有哪些损坏的块等[校验码的机制]），启动完成之后，一般每隔1h，datanode都会向namenode汇报块的状态报告。



[注]：设置块的大小和查看datanode上的数据块：

```shell
#增加如下参数项(默认为128M)
$ vim hdfs-site.xml
<property>
	<name>dfs.blocksize</name>
	<value>134217728</value>
</property>

#datanode节点存储的数据块：
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/data/current/BP-1491715413-172.16.127.129-1502205769566/current/finalized$ du -sh *
4.0K    blk_1073741826
4.0K    blk_1073741826_1002.meta
4.0K    blk_1073741833
4.0K    blk_1073741833_1009.meta
36K     blk_1073741835
4.0K    blk_1073741835_1011.meta
96K     blk_1073741836
4.0K    blk_1073741836_1012.meta
48K     blk_1073741837
4.0K    blk_1073741837_1013.meta
```

<br>



### YARN初步简介

`两个功能`：集群资源的管理和分配、多任务的调度

`架构设计`：主从架构

* 主节点（resourcemanager）
* 从节点（nodemanager）

`YRAN运行流程`：

* 客户端向resourcemanager提交应用申请，resourcemanager会为每个任务生成并启动一个appMaster；
* 应用管理者appMaster判断mapreduce任务需要多少资源，然后向resourcemanager申请资源；
* resourcemanager为每个MR appMaster返回一个Container容器，然后应用在返回的Container容器运行；
* mapreduce任务的运行和关闭，由appMaster负责监控，MR任务运行完毕之后，appMaster将关闭释放资源

相关组件的说明：

```shell
#resourcemanager
处理客户端的请求
启动和监控applicationMaster
监控nodemanager
资源的分配与调度

#nodemanager
单个节点上的资源管理
处理来自resourcemanager的命令
处理来自applicationMaster的命令

#applicationMaster
数据切分
为应用程序申请资源，并分配给内部任务
任务监控和容错

#container
对任务环境的抽象，封装了CPU、内存等多维资源以及环境变量，启动变量等任务运行相关的信息
```

<br>



### hadoop分布式的三种模式

- [Local (Standalone) Mode](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/#Standalone_Operation)：本地模式，使用的是本地文件系统，调试使用，[配置方法](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation)
- [Pseudo-Distributed Mode](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/#Pseudo-Distributed_Operation)：伪分布式，使用的是HDSF，调试使用，[配置方法](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)，[参考项](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)
- [Fully-Distributed Mode](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/#Fully-Distributed_Operation)：完全分布式，使用的是HDSF，生产使用，[配置方法](http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation)

<br>



> 常见的hadoop的默认web端口：
>
> yarn外部管理界面端口：8088
>
> HDFS外部管理界面端口：50070
>
> secondarynamenode外部管理界面端口：50090

<br>



安装前准备工作：

* 虚拟机(使用virtualbox 或 vmware，此处我们使用virtualbox)

  ```default
  1. virtualbox新建三台虚拟机，即nimbusz、supervisor01z、supervisor02z
  2. 每台虚拟机有两个网卡(也可以只使用，但每个虚拟机都会占用路由器ip资源)
     NAT网卡用于连接外网(但不能实现物理机和虚拟机以及虚拟机之间的通信，注意勾选高级-混杂模式-全部允许)，
     host-only网卡用于物理机和虚拟机以及虚拟机之间的通信(但不能连接外网)
  3. ifconfig大致如下：
  	eth0      Link encap:Ethernet  HWaddr 08:00:27:72:f1:95  
            	  inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
                ...
  	eth1      Link encap:Ethernet  HWaddr 08:00:27:8d:c7:a8  
                inet addr:192.168.56.102  Bcast:192.168.56.255  Mask:255.255.255.0
                ...
  	lo        Link encap:Local Loopback  
                inet addr:127.0.0.1  Mask:255.0.0.0
                ...
  4. hosts配置大致如下：
    127.0.0.1      localhost
    192.168.56.101 dbserver
    192.168.56.102 nimbusz
    192.168.56.103 supervisor01z
    192.168.56.104 supervisor02z
  ```


* 源码编译hadoop获取native或CHD-hadoop对应版本的native本地包
* 用新获取的${HADOOP_HOME}/lib/native 替换 apache hadoop的native

```shell
#####源码编译hadoop获取native
# 1. 下载并安装 snappy库(e.g.snappy-1.1.4.tar.gz)
.configure
make
sudo make install

# 2. 用maven编译hadoop原文件，编译生成的文件在target目录下
$ mvn package -Pdist,native -DskipTests -Dtar -Drequire.snappy 

# 3. 用编译而成的 xxx/target/hadoop-2.5.0/lib/native 
# 替换hadoop自己的 native文件夹 （hadoop默认的安装包不支持native）	 

# 4. 再次检查hadoop是否支持snappy压缩
$ bin/hadoop checknative


#####CHD-hadoop对应版本的native本地包
# 1. 下载，解压rpm包(以hadoop-2.5.0 x64举例)
wget http://archive.cloudera.com/cdh5/redhat/5/x86_64/cdh/5.2.3/RPMS/x86_64/hadoop-2.5.0+cdh5.2.3+615-1.cdh5.2.3.p0.15.el5.x86_64.rpm
# 2. 替换
# 3. 检查
```

下面以hadoop伪分布式模式的配置和启动过程进行大致讲解：

```shell
# 解压hadoop后的hadoop的目录，bin  etc  include  lib  libexec  sbin  share
# 建议，这里share文件夹下面有一个doc文件夹，存放的是hadoop的文档，由于线上不用，而且该文件夹很大，故删除
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0$ du -sh *
...
1.7G    share
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0$ rm -rf share/doc


#伪分布式配置方法：
#1. 修改配置文件的 $JAVA_HOME 项，$JAVA_HOME需要写具体的java安装home路径而不是$JAVA_HOME
hadoop-env.sh
mapred-env.sh
yarn-env.sh

#2. 查看当前的配置的配置文件是否生效
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0$ bin/hadoop


#3. 配置文件
#etc/hadoop/core-site.xml: 
<configuration>
	#hdfs主从节点通信默认端口号为8020
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://nimbusz:8020</value>
    </property>
    #hdfs的namenode节点元数据的镜像文件
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/modules/hadoop-2.5.0/data/tmp</value>
    </property>
</configuration>

#etc/hadoop/hdfs-site.xml:  #为分布式，副本数为1
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

#etc/hadoop/slaves
#配置nodemanager配置文件slaves
#决定了datanode和nodemanager所在机器，一行就是一个主机，此处只有一个node节点
#严格来讲，slaves文件存储的并不是真正的datanode节点，它唯一的作用是在集群操作的时候要正对这个文件去ssh处理。我们在start-all.sh、start-dfs.sh、start-yarn.sh的时候，通过ssh向远程服务器发送指令，将远程服务器启动。至于节点是不是datanode节点，取决于dfs.hosts和dfs.hosts.exclude两个参数。
nimbusz


#4. 格式化hdfs文件系统，并查看格式化后生成的元数据镜像文件(刚才配置在core-site.xml中)
#注意，格式化的时候需要保证fsimage不存在，这样新生成的fsimage镜像文件才不会和旧的发生冲突
ubuntu@hadoop01:/opt/modules/hadoop-2.5.0$ bin/hdfs namenode -format
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/name/current$ ls
fsimage_0000000000000000000  fsimage_0000000000000000000.md5  seen_txid  VERSION


#5. 启动namenode和datanode
sbin/hadoop-daemon.sh start namenode
sbin/hadoop-daemon.sh start datanode
jps
10920 NameNode
11003 DataNode
11071 Jps


#6. 打开hdfs web 管理界面，端口号默认为50070，可通过hdfs-site dfs.namenode.http-address 选项配置
http://nimbusz:50070

#7. 在hdfs文件系统创建文件目录，约定用户目录格式类似于 /user/ubuntu
bin/hdfs dfs -mkdir -p /user/ubuntu

#8. 在hdfs文件系统中循环查看文件及文件目录
bin/hdfs dfs -ls -R /
drwxr-xr-x   - ubuntu supergroup          0 2017-08-08 20:33 /user
drwxr-xr-x   - ubuntu supergroup          0 2017-08-08 20:33 /user/ubuntu

#9. 上传下载查看文件
touch /opt/datas/test.txt
bin/hdfs dfs -put /opt/datas/test.txt /user/ubuntu/test.txt
bin/hdfs dfs -get /user/ubuntu/test.txt /opt/datas/test01.txt
bin/hdfs dfs -cat /user/ubuntu/test.txt


#10. 伪分布式（单节点）运行在yarn上面需要的配置：
#etc/hadoop/yarn-site.xml:
<configuration>
    #指定resourcemanager所在的机器
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>nimbusz</value>
    </property>
    #启动mapreduce_shuffle服务以支持运行mapreduce程序
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
#etc/hadoop/mapred-site.xml:
<configuration>
	#指定mapreduce运行在yarn上
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>


#11. 启动yarn的 resourcemanager 和 nodemanager
sbin/yarn-daemon.sh start resourcemanager
sbin/yarn-daemon.sh start nodemanager
jps
12339 Jps
12004 ResourceManager
12248 NodeManager
10920 NameNode
11003 DataNode

#12. 浏览器访问yarn客户端管理界面 可以在yarn-site.xml的 yarn.resourcemanager.webapp.address 配置
#默认的客户端管理界面http地址 ${yarn.resourcemanager.hostname}:8088
http://nimbusz:8088


#13. 测试在yarn上跑一个hadoop自带的wordcount程序
#前提是在core-site.xml文件中配置fs.defaultFS的值默认为hdfs文件系统
#上传一个待计数的wc.txt文件到hdfs文件系统
bin/hdfs dfs -mkdir -p /user/ubuntu/wc
bin/hdfs dfs -put /opt/datas/wc.txt /user/ubuntu/wc/wc.txt
#在yarn上启动wordcount程序(注意输出目录开始时不能存在)
bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount /user/ubuntu/wc /user/ubuntu/wc/output

#查看运行结果
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0$ bin/hdfs dfs -text /user/ubuntu/wc/output/part*
hadoop  4
hello   2
hive    1
mapreduce       1
spark   1
world   1
yarn    1
```

<br>



### hadoop日志文件

启动日志文件目录：$HADOOP_HOME/logs

分析日志文件的格式（框架名-用户名-进程-主机-后缀）：

* log：通过log4j记录的大部分应用程序的日志信息
* out：记录标准输出和标准错误日志，少量记录

```shell
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/logs$ du -sh *
48K     hadoop-ubuntu-datanode-nimbusz.log
4.0K    hadoop-ubuntu-datanode-nimbusz.out
92K     hadoop-ubuntu-namenode-nimbusz.log
8.0K    hadoop-ubuntu-namenode-nimbusz.out
0       SecurityAuth-ubuntu.audit
72K     userlogs
52K     yarn-ubuntu-nodemanager-nimbusz.log
4.0K    yarn-ubuntu-nodemanager-nimbusz.out
108K    yarn-ubuntu-resourcemanager-nimbusz.log
4.0K    yarn-ubuntu-resourcemanager-nimbusz.out
4.0K    yarn-ubuntu-resourcemanager-nimbusz.out.1
4.0K    yarn-ubuntu-resourcemanager-nimbusz.out.2
```

<br>



### hadoop自带的作业历史记录服务

作业历史记录服务会在应用完成运行之后，会将日志的信息上传到HDFS文件系统上（一般保存在HDFS的 /tmp 目录下）。如果没有配置和开启历史服务器的服务，在yarn的web管理页是看不到mapreduce任务的history项的。

配置和开启mr任务历史服务器的过程如下：

```shell
#配置
#增加如下两个参数
$ vim mapred-site.xml
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>nimbusz:10020</value>
</property>
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>nimbusz:19888</value>
</property>

#启动
$ sbin/mr-jobhistory-daemon.sh start historyserver
jps
13443 JobHistoryServer #
12004 ResourceManager
12248 NodeManager
10920 NameNode
13528 Jps
11003 DataNode
```

<br>



### hadoop的日志聚集功能

如果没有配置hadoop的日志聚集功能，则在yarn点击某个mr任务 map或reduce 的 logs 选项是看不到日志信息的。

```shell
Aggregation is not enabled. Try the nodemanager at nimbusz:42720
```

配置和开启hadoop的日志聚集功能：

```shell
#配置
$ vim yarn-site.xml 
<property>
	#启用hadoop的日志聚集功能(默认false)
	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
</property>
<property>
	#日志保存在HDFS上的期限(默认为-1，这里配置的是一周)
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
</property>


#重启yarn
sbin/yarn-daemon.sh stop resourcemanager 
sbin/yarn-daemon.sh stop nodemanager
sbin/mr-jobhistory-daemon.sh stop historyserver
#
sbin/yarn-daemon.sh start resourcemanager 
sbin/yarn-daemon.sh start nodemanager
sbin/mr-jobhistory-daemon.sh start historyserver


#重新在yarn上运行mr wordcount程序，查看作业历史记录服务和日志聚集功能是否生效
bin/hdfs dfs -rm -r /user/ubuntu/wc/output
bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount /user/ubuntu/wc /user/ubuntu/wc/output
```

<br>



### hadoop HDFS 权限检查

比方说刚才我们配置了hadoop的作业历史记录服务，一般会将日志信息保存在HDFS的 /tmp 目录下，但是在[web页](http://nimbusz:50070/explorer.html#/)没有配置权限情况下我们是看不到 tmp里面的内容的：

```shell
Permission denied: user=dr.who, access=READ_EXECUTE, inode="/tmp":ubuntu:supergroup:drwx------
```

原因是因为在 hdfs-site.xml  配置中 dfs.permissions.enabled 参数默认是true的，在开发环境，为了方便查看日志，通常先设置为false：

```shell
#配置
$ vim hdfs-site.xml
<property>
	<name>dfs.permissions.enabled</name>
	<value>false</value>
</property>

$ vim core-site.xml
<property>
	#设置一个hadoop的web静态用户名(默认为dr.who)
	<name>hadoop.http.staticuser.user</name>
	<value>ubuntu</value>
</property>


#重启hadoop相关度的所有进程
sbin/yarn-daemon.sh stop resourcemanager
sbin/yarn-daemon.sh stop nodemanager
sbin/mr-jobhistory-daemon.sh stop historyserver
sbin/hadoop-daemon.sh stop namenode
sbin/hadoop-daemon.sh stop datanode

sbin/yarn-daemon.sh start resourcemanager
sbin/yarn-daemon.sh start nodemanager
sbin/mr-jobhistory-daemon.sh start historyserver
sbin/hadoop-daemon.sh start namenode
sbin/hadoop-daemon.sh start datanode

jps
15617 Jps
15490 NameNode
15043 ResourceManager
15547 DataNode
15293 NodeManager
15439 JobHistoryServer

#重新在yarn上运行mr wordcount程序，查看权限检查是否生效
bin/hdfs dfs -rm -r /user/ubuntu/wc/output
bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0.jar wordcount /user/ubuntu/wc /user/ubuntu/wc/output
```

<br>



### 启用HDFS secondarynamenode

namenode元数据（文件名、文件目录结构、文件生成时间、副本数、文件权限等信息）初始的时候是通过 bin/hdfs namenode -format 生成的，通过格式化这条命令，会在${HADOOP_TMP_DIR}/dfs/name/current 的 目录中生成一个`fsimage_*`的镜像文件（存放元数据），在namenode节点启动以后，元数据就被加载到内存当中。当在HDFS系统中做了增删改的操作之后，同样会在${HADOOP_TMP_DIR}/dfs/name/current 目录中产生`edits_*` 操作日志文件。namenode节点在重启的时候，会先读取 fsimage 和 edits文件，以将文件系统的元数据加载到namenode节点的内存当中。

```shell
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/name/current$ du -sh *
1.0M    edits_0000000000000000001-0000000000000000130
1.0M    edits_inprogress_0000000000000000131
4.0K    fsimage_0000000000000000000
4.0K    fsimage_0000000000000000000.md5
4.0K    seen_txid
4.0K    VERSION
```

通常如果hadoop如果运行了很长时间之后，edits操作日志文件就会变得很大。当这时候因为修改了hadoop的某个配置项或服务器修复需要重启 namenode节点的时候，这个重启的过程就会变得很慢（因为读edits操作日志文件通常要比读fsimage镜像文件慢很多）。所以`为了减少namenode重启的时间`，这时候就需要HDFS 的secondarynamenode。

1. secondarynamenode在启动以后也会每隔一段时间读取 fsimage镜像文件 和 操作日志文件，读取完成之后，这些元数据信息会被加载到 secondarynamenode 节点自己的内存当中，然后将自己内存当中的元数据信息写到新的 fsimage镜像文件当中。
2. 当有新的HDFS的增删改操作之后，secondarynamenode会产生新的edits操作日志。然后下一次重启的时候，又会读取新的 fsimage镜像文件 和 操作日志文件，类似于上述1的步骤。

通过以上两步，secondarynamenode会不断合并edits操作日志文件到新的fsimage镜像文件，这样就有效地减少了namenode重启所花费的时间。

用一句话归结secondarynamenode的作用就是：每隔一段时间合并生成HDFS元数据的新的快照，以减少namenode重启所花费的时间。

以下是配置和启动secondarynamenode服务的过程：

```shell
#配置
#增加如下参数项配置
$ vim hdfs-site.xml 
<property>
	<name>dfs.namenode.secondary.http-address</name>
	<value>nimbusz:50090</value>
</property>


#启动
sbin/hadoop-daemon.sh start secondarynamenode
jps
19685 NodeManager
19862 NameNode
20583 Jps
19784 JobHistoryServer
19437 ResourceManager
20542 SecondaryNameNode #
19967 DataNode


#访问web管理页
http://nimbusz:50090


#查看secondarynamenode生成的文件(除了namenode文件夹，又多出了一个namesecondary文件夹)
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs$ ls
data  name  namesecondary

ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/namesecondary/current$ du -sh *
1.0M    edits_0000000000000000001-0000000000000000130
4.0K    edits_0000000000000000131-0000000000000000132
4.0K    fsimage_0000000000000000000
4.0K    fsimage_0000000000000000000.md5
4.0K    fsimage_0000000000000000132
4.0K    fsimage_0000000000000000132.md5
4.0K    VERSION
```

<br>



### hadoop的快捷启动和快捷关闭脚本

```shell
# sbin/start-dfs.sh
# 快捷启动 namenode、datanode、secondarynamenode
$ sbin/start-dfs.sh 
Starting namenodes on [nimbusz]
ubuntu@nimbusz's password: 
nimbusz: starting namenode, logging to xx/logs/hadoop-ubuntu-namenode-nimbusz.out
ubuntu@nimbusz's password: 
nimbusz: starting datanode, logging to xx/logs/hadoop-ubuntu-datanode-nimbusz.out
Starting secondary namenodes [nimbusz]
ubuntu@nimbusz's password: 
nimbusz: starting secondarynamenode, logging to xx/logs/hadoop-ubuntu-secondarynamenode-nimbusz.out


#上述快捷启动的过程中，我们可以看到，虽然启动方便了很多，但是每次都要输入登录密码，挺麻烦的。
#这时候就可以设置不同主机之间通过ssh免密码登录。
#所谓的 ssh公钥登录 就是用户将自己的公钥存储在远程主机上，登录的时候远程主机会向用户发送一段随机的字符串，
#用户用自己的私钥加密后再发回到远程主机，远程主机主机用事先存储的公钥进行解密，如果成功，就证明用户是可信的
#直接允许登录shell，不再要求输入密码。
#在用户机器上生成秘钥-公钥对
ssh-keygen -t [rsa|dsa] -C "message…"
#拷贝用户机器上的公钥到需要登录的远程主机上
#第一次密码验证成功后会将公钥内容拷贝到远程主机的authorized_keys文件
ssh-copy-id ubuntu@nimbusz


#再次使用dfs快捷启动脚本就不会每次都输密码那么麻烦了
$ sbin/start-dfs.sh 
$ sbin/stop-dfs.sh
Stopping namenodes on [nimbusz]
nimbusz: stopping namenode
nimbusz: stopping datanode
Stopping secondary namenodes [nimbusz]
nimbusz: stopping secondarynamenode


#快捷关闭和启动yarn的resourcemanager和nodemanager
$ sbin/stop-yarn.sh 
$ sbin/start-yarn.sh 

#剩下的 MR作业历史记录服务 需要单独关闭和开启
$ sbin/mr-jobhistory-daemon.sh stop historyserver
$ sbin/mr-jobhistory-daemon.sh start historyserver
```

<br>



### hadoop启动常见的问题

1) 多次格式化导致启动失败

```shell
#格式化hdfs文件系统，并查看格式化后生成的元数据镜像文件($HADOOP_TMP_DIR配置在core-site.xml中)
#注意，格式化的时候需要保证fsimage不存在，这样新生成的fsimage镜像文件才不会和旧的发生冲突
ubuntu@hadoop01:/opt/modules/hadoop-2.5.0$ bin/hdfs namenode -format

ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/name/current$ ls
fsimage_0000000000000000000  fsimage_0000000000000000000.md5  seen_txid  VERSION
```

2) 集群版本不一致导致的启动失败

```shell
#一般namenode、secondarynamenode、datanode三者的集群id，即clusterID不一致可能启动就会出现问题
ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/name/current$ cat VERSION 
namespaceID=389815234
clusterID=CID-0ead1360-19df-4a73-9fc3-8ff2ecc8c472
cTime=0
storageType=NAME_NODE
blockpoolID=BP-1491715413-172.16.127.129-1502205769566
layoutVersion=-57


ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/namesecondary/current$ cat VERSION 
namespaceID=389815234
clusterID=CID-0ead1360-19df-4a73-9fc3-8ff2ecc8c472
cTime=0
storageType=NAME_NODE
blockpoolID=BP-1491715413-172.16.127.129-1502205769566
layoutVersion=-57


ubuntu@nimbusz:/opt/modules/hadoop-2.5.0/data/tmp/dfs/data/current$ cat VERSION 
storageID=DS-89f31c8e-49b0-4614-8453-f59796c88adb
clusterID=CID-0ead1360-19df-4a73-9fc3-8ff2ecc8c472
cTime=0
datanodeUuid=59840327-bd85-4aa3-9b87-466724b63310
storageType=DATA_NODE
layoutVersion=-55
```

3) 端口号被占用

```shell
#使用netstat查看端口号是否被占用
netstat -lntp[u]
```

4) 多用户的混用，造成进程冲突（pid）

```shell
#这只是针对的单节点的为分布式的启动模式而言的
#这时候需要把 /tmp目录下的 *.pid文件删除，然后重新以一个用户启动
```

<br>



### 其他一些hadoop常用的shell命令

```shell
#下载两个文件并合并到本地的一个文件中
bin/hdfs dfs -getmerge xxx/file01 xxx/file02 /opt/datas/targetfile.txt

#查看当前hdfs是不是处于安全状态(安全状态是只读的)
bin/hdfs dfsadmin -safemode get
#进入|离开安全模式
bin/hdfs dfsadmin -safemode [enter|leave]
```



